<!DOCTYPE html>
<html><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>ii.nz</title>
   <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Yeseva+One&family=Noto+Sans">
   <link rel="stylesheet" href='/css/normalize.css' type="text/css"/>
   <link rel="stylesheet" href='/css/variables.css' type="text/css"/>
   <link rel="stylesheet" href='/css/main.css' type="text/css"/>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=DM+Sans:opsz,wght@9..40,500&display=swap" rel="stylesheet">
   <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
  </style>
</head>
<body>
        <body onload="checkDarkMode()">
    <header>
      <nav class="navbar" role="navigation">
        <div class="navbar__left">
          <a href="/" class="nav-logo">
            <img src="/assets/nav-logo.png" id="navLogo" alt="Logo">
          </a>
        </div>
        <div class="navbar__right" id="navbarRight">
          <li>   
            <div class="icon" onclick="toggleIcon()">
              <i class="fas fa-moon" id="toggleMode"></i>
            </div>
          </li>
          <a href="/team">Team</a>
          <a href="/projects">Projects</a>
          <a href="/post">Blog</a>
          <a href="/contact">Contact</a>
        </div>
      </nav>
    </header>
  
    <script>
      
      function checkDarkMode() {
        var storedMode = localStorage.getItem('darkMode');
        if (storedMode === 'enabled') {
          applyDarkMode();
        }
      }
  
      function toggleIcon() {
        var iconElement = document.getElementById('toggleMode');
     
        
        if (iconElement.classList.contains('fa-moon')) {
          applyDarkMode();
          localStorage.setItem('darkMode', 'enabled');
        } else {
          removeDarkMode();
          localStorage.setItem('darkMode', 'disabled');
        }
      }
  
      
      function applyDarkMode() {
        var iconElement = document.getElementById('toggleMode');
        var bg = document.getElementById("content");
        var header = document.getElementsByTagName('header')[0];
        var navbarRight = document.getElementById('navbarRight');
        var headings = document.querySelectorAll('h1, h2, h3, h4, h5, h6, p, b, label,li');
        var logoImage = document.getElementById('navLogo')
        var blogPosts = document.getElementsByClassName('post-list__item');
        var modals = document.getElementsByClassName('modal-content');


        iconElement.classList.remove('fa-moon');
        iconElement.classList.add('fa-sun');
        bg.classList.add('bg-dark');
        header.classList.add('header-dark');
        navbarRight.classList.add("navbar__right--dark");
  
        for (var i = 0; i < headings.length; i++) {
          headings[i].style.color = 'white';
        }
        for(var i = 0; i < blogPosts.length; i++){
            blogPosts[i].classList.add('blog-dark');
        }
        for(var i = 0; i < modals.length; i++){
            modals[i].classList.add('modal-dark');
        }
        logoImage.src = "\/assets\/dark-mode-logo.png";
      }
  
      
      function removeDarkMode() {
        var iconElement = document.getElementById('toggleMode');
        var bg = document.getElementById("content");
        var header = document.getElementsByTagName('header')[0];
        var navbarRight = document.getElementById('navbarRight');
        var headings = document.querySelectorAll('h1, h2, h3, h4, h5, h6, p, b, label,li');
        var logoImage = document.getElementById('navLogo')
        var blogPosts = document.getElementsByClassName('post-list__item');
        var modals = document.getElementsByClassName('modal-content');

        
        iconElement.classList.remove('fa-sun');
        iconElement.classList.add('fa-moon');
        bg.classList.remove('bg-dark');
        header.classList.remove('header-dark');
        navbarRight.classList.remove("navbar__right--dark");
        logoImage.src = "\/assets\/nav-logo.png";
  
        for (var i = 0; i < headings.length; i++) {
          headings[i].style.color = '';
        }
        for(var i = 0; i < blogPosts.length; i++){
            blogPosts[i].classList.remove('blog-dark');
        }
        for(var i = 0; i < modals.length; i++){
            modals[i].classList.remove('modal-dark');
        }

      }
    </script>
  </body>
  
        <main id="content">
<section class="post__entry">
  <article>
    <h1 class="post__title">Building a data pipeline for displaying Kubernetes public artifact traffic</h1>
    <div class="post__info">
      <p>By
        
        <span class='post__author'>
          Caleb Woodbine
        </span>
        
        ,
        <time>August 24, 2021</time>
      </p>
    </div>
    </div>
    <div class="post__content">
      
<h2 id="introduction">
  Introduction
  <a class = 'heading-anchor' href="#introduction">#</a>
</h2>
<p>ii is a member of the <a href="https://github.com/kubernetes/community/blob/master/wg-k8s-infra/README.md">Kubernetes Infra working group</a>, the group responsible for defining and managing the infrastructure for the Kubernetes project.
The infrastructure includes but is not limited to:</p>
<ul>
<li><a href="https://prow.k8s.io">prow.k8s.io</a></li>
<li><a href="https://github.com/kubernetes/k8s.io/tree/main/apps/slack-infra">Slack infra</a></li>
<li><a href="https://github.com/kubernetes/k8s.io/tree/main/dns">DNS records</a></li>
<li><a href="https://github.com/kubernetes/k8s.io/blob/main/apps/k8s-io/README.md">k8s.io redirects</a></li>
<li>project and resource management for SIGs and WGs</li>
</ul>
<p>One of goals of the group is to discover where the costs are coming from and encorage large traffic users to self-host / cache the artifacts on their side, as well as to spend the funds better for the entirety of the Kubernetes infrastructure.
In order to do this, we first need to discover where the traffic is coming from.</p>
<p>With access to a bucket containing the logs of traffic over a certain time period, we&rsquo;re able to parse it and calculate a few things.</p>
<p>Organisations publish and advertise their IP addresses through <a href="https://en.wikipedia.org/wiki/Border%5FGateway%5FProtocol">BGP</a>, a fundamental sub-sytem of the internet.
IP addresses are published in blocks (subnetted) in <a href="https://en.wikipedia.org/wiki/Autonomous%5Fsystem%5F%28Internet%29">ASN</a> data.
With that in mind, the ASN data of cloud-providers and organisations that fund the CNCF for Kubernetes are publically available, through this we&rsquo;re able to figure out the Kubernetes Infra project should communicate to about their traffic usage.</p>

<h2 id="considering-steps">
  Considering steps
  <a class = 'heading-anchor' href="#considering-steps">#</a>
</h2>
<p>At the beginning, the pieces of this puzzle were less known so it was considered to be something like this</p>
<figure>
<img alt='asn-data-pipeline-plan' src='/images/2021/asn-data-pipeline-plan.svg'>
</figure>

<h2 id="planning-out-the-pipeline">
  Planning out the pipeline
  <a class = 'heading-anchor' href="#planning-out-the-pipeline">#</a>
</h2>
<p>After some more research and discovery, here is the pipeline architecture</p>
<figure class="oversized">
<img alt='asn-data-pipeline' src='/images/2021/asn-data-pipeline.svg'>
</figure>
<p>Our data sources are:</p>
<ul>
<li>global ASN data (<a href="https://bgp.potaroo.net/">Potaroo</a>, <a href="https://github.com/hadiasghari/pyasn">PyASN</a>)</li>
<li>ASN-to-vendor metadata YAML</li>
<li>Kubernetes public artifact logs</li>
</ul>
<p>ii is a heavy user of Postgres for data transformation and processing (see <a href="https://github.com/cncf/apisnoop/tree/main/apps/snoopdb">APISnoop and SnoopDB</a>).
Using these sources, we&rsquo;ll do processing in a Postgres container and in BigQuery.
We&rsquo;re using BigQuery, since we&rsquo;ll need to display the data in DataStudio for our <a href="https://github.com/kubernetes/community/blob/master/wg-k8s-infra/README.md#meetings">bi-weekly Thursday (NZT) wg-k8s-infra meetings</a>.</p>
<p>The data will need to be reproducible via the job running once again.</p>

<h2 id="finding-asn-databases">
  Finding ASN databases
  <a class = 'heading-anchor' href="#finding-asn-databases">#</a>
</h2>
<p>ASN data is important for this operation, because it allows us to find who owns the IP addresses which are making requests to the artifact servers via the logs.
All ASN data is public, which makes it easy to discover these IP blocks from ASNs and match them to their known owner.</p>
<p>In the pipeline, PyASN is used with some custom logic to parse ASN data provided by the Potaroo ASN database (which could be any provider).</p>

<h2 id="the-pipeline-of-the-asn-and-ip-to-vendor-discussion">
  The pipeline of the ASN and IP to vendor discussion
  <a class = 'heading-anchor' href="#the-pipeline-of-the-asn-and-ip-to-vendor-discussion">#</a>
</h2>
<figure class="oversized">
<img alt='asn-ip-pipeline' src='/images/2021/asn-ip-pipeline.svg'>
</figure>

<h2 id="gathering-the-asns-of-cncf-supporting-organisations">
  Gathering the ASNs of CNCF supporting organisations
  <a class = 'heading-anchor' href="#gathering-the-asns-of-cncf-supporting-organisations">#</a>
</h2>
<p>There are quite a number of organisations that support the CNCF and consume Kubernetes artifacts.
After considering a handful of organsations to begin with, the ASNs are discovered through sites like <a href="https://www.peeringdb.com/">PeeringDB</a> and committing into some files in this repo
<a href="https://github.com/kubernetes/k8s.io/tree/main/registry.k8s.io/infra/meta/asns">k8s.io/registry.k8s.io/infra/meta/asns</a>
for later parsing and reading.</p>
<p>The ASNs are reviewed and verified by a member of the organisation that ii has a relationship with or someone through the relationship. Some organisations may not wish to verify this collected public data, in that case we will just trust it.</p>
<p>The metadata file will also contain directions on a later service to possibly handle the redirections to the closest cloud-provider, based on the request.</p>

<h2 id="kubernetes-public-artifact-logs">
  Kubernetes Public Artifact Logs
  <a class = 'heading-anchor' href="#kubernetes-public-artifact-logs">#</a>
</h2>
<p>A GCP project in the Kubernetes org was provisioned to house the PII (Publicly Identifing Information) logs of the <a href="https://cloud.google.com/container-registry/">GCR</a> logs and artifacts logs in a <a href="https://cloud.google.com/storage">GCS</a> bucket.</p>

<h2 id="postgres-processing">
  Postgres processing
  <a class = 'heading-anchor' href="#postgres-processing">#</a>
</h2>
<p>The largest part of the data transformation happens in a Postgres Pod, running as a Prow Job.
Firstly, we bring up Postgres and begin to run the pg-init scripts.</p>
<p>A dataset is created in the <em>kubernetes-public-ii-sandbox</em> project, Potaroo pre-processed data is downloaded along with PyASN data, and PyASN data is converted for use.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:2;-o-tab-size:2;tab-size:2;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>bq mk <span style="color:#4070a0;font-weight:bold">\
</span></span></span><span style="display:flex;"><span><span style="color:#4070a0;font-weight:bold"></span>    --dataset <span style="color:#4070a0;font-weight:bold">\
</span></span></span><span style="display:flex;"><span><span style="color:#4070a0;font-weight:bold"></span>    --description <span style="color:#4070a0">&#34;etl pipeline dataset for ASN data from CNCF supporting vendors of k8s infrastructure&#34;</span> <span style="color:#4070a0;font-weight:bold">\
</span></span></span><span style="display:flex;"><span><span style="color:#4070a0;font-weight:bold"></span>    <span style="color:#4070a0">&#34;</span><span style="color:#70a0d0">${</span><span style="color:#bb60d5">GCP_PROJECT</span><span style="color:#70a0d0">}</span><span style="color:#4070a0">:</span><span style="color:#70a0d0">${</span><span style="color:#bb60d5">GCP_BIGQUERY_DATASET</span><span style="color:#70a0d0">}</span><span style="color:#4070a0">_</span><span style="color:#007020;font-weight:bold">$(</span>date +%Y%m%d<span style="color:#007020;font-weight:bold">)</span><span style="color:#4070a0">&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#60a0b0;font-style:italic"># ...</span>
</span></span><span style="display:flex;"><span>gsutil cp gs://ii_bq_scratch_dump/potaroo_company_asn.csv  /tmp/potaroo_data.csv
</span></span><span style="display:flex;"><span><span style="color:#60a0b0;font-style:italic"># ...</span>
</span></span><span style="display:flex;"><span>pyasn_util_download.py --latest
</span></span><span style="display:flex;"><span><span style="color:#60a0b0;font-style:italic"># ...</span>
</span></span><span style="display:flex;"><span>python3 /app/ip-from-pyasn.py /tmp/potaroo_asn.txt ipasn_latest.dat /tmp/pyAsnOutput.csv
</span></span></code></pre></div><p>(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#get-dependencies">stage get dependencies</a>)</p>
<p>Tables are created so that the data can be stored and processed.
(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#migrate-schemas">migrate schemas</a>)</p>
<p>Data is now loaded from the local ASN data and outputted as CSV for use shortly.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:2;-o-tab-size:2;tab-size:2;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>copy (select * from pyasn_ip_asn_extended) to &#39;/tmp/pyasn_expanded_ipv4.csv&#39; csv header;
</span></span></code></pre></div><p>(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#load-pyasn-data-into-postgres">stage load PyASN data into Postgres</a>)</p>
<p>The data is now uploaded to ASN BigQuery for later use.</p>
<p>Next, the vendor ASN metadata is downloaded from GitHub</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:2;-o-tab-size:2;tab-size:2;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">for</span> VENDOR in <span style="color:#70a0d0">${</span><span style="color:#bb60d5">VENDORS</span>[*]<span style="color:#70a0d0">}</span>; <span style="color:#007020;font-weight:bold">do</span>
</span></span><span style="display:flex;"><span>  curl -s <span style="color:#4070a0">&#34;https://raw.githubusercontent.com/kubernetes/k8s.io/main/registry.k8s.io/infra/meta/asns/</span><span style="color:#70a0d0">${</span><span style="color:#bb60d5">VENDOR</span><span style="color:#70a0d0">}</span><span style="color:#4070a0">.yaml&#34;</span> <span style="color:#4070a0;font-weight:bold">\
</span></span></span><span style="display:flex;"><span><span style="color:#4070a0;font-weight:bold"></span>      | yq e . -j - <span style="color:#4070a0;font-weight:bold">\
</span></span></span><span style="display:flex;"><span><span style="color:#4070a0;font-weight:bold"></span>      | jq -r <span style="color:#4070a0">&#39;.name as $name | .redirectsTo.registry as $redirectsToRegistry | .redirectsTo.artifacts as $redirectsToArtifacts | .asns[] | [. ,$name, $redirectsToRegistry, $redirectsToArtifacts] | @csv&#39;</span> <span style="color:#4070a0;font-weight:bold">\
</span></span></span><span style="display:flex;"><span><span style="color:#4070a0;font-weight:bold"></span>        &gt; <span style="color:#4070a0">&#34;/tmp/vendor/</span><span style="color:#70a0d0">${</span><span style="color:#bb60d5">VENDOR</span><span style="color:#70a0d0">}</span><span style="color:#4070a0">_yaml.csv&#34;</span>
</span></span><span style="display:flex;"><span>  bq load --autodetect <span style="color:#4070a0">&#34;</span><span style="color:#70a0d0">${</span><span style="color:#bb60d5">GCP_BIGQUERY_DATASET</span><span style="color:#70a0d0">}</span><span style="color:#4070a0">_</span><span style="color:#007020;font-weight:bold">$(</span>date +%Y%m%d<span style="color:#007020;font-weight:bold">)</span><span style="color:#4070a0">.vendor_yaml&#34;</span> <span style="color:#4070a0">&#34;/tmp/vendor/</span><span style="color:#70a0d0">${</span><span style="color:#bb60d5">VENDOR</span><span style="color:#70a0d0">}</span><span style="color:#4070a0">_yaml.csv&#34;</span> asn_yaml:integer,name_yaml:string,redirectsToRegistry:string,redirectsToArtifacts:string
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">done</span>
</span></span></code></pre></div><p>along with the IP blocks from major cloud-providers and uploaded to BigQuery</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:2;-o-tab-size:2;tab-size:2;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>curl <span style="color:#4070a0">&#34;https://download.microsoft.com/download/7/1/D/71D86715-5596-4529-9B13-DA13A5DE5B63/ServiceTags_Public_20210802.json&#34;</span> <span style="color:#4070a0;font-weight:bold">\
</span></span></span><span style="display:flex;"><span><span style="color:#4070a0;font-weight:bold"></span>    | jq -r <span style="color:#4070a0">&#39;.values[] | .properties.platform as $service | .properties.region as $region | .properties.addressPrefixes[] | [., $service, $region] | @csv&#39;</span> <span style="color:#4070a0;font-weight:bold">\
</span></span></span><span style="display:flex;"><span><span style="color:#4070a0;font-weight:bold"></span>      &gt; /tmp/vendor/microsoft_raw_subnet_region.csv
</span></span><span style="display:flex;"><span>curl <span style="color:#4070a0">&#39;https://www.gstatic.com/ipranges/cloud.json&#39;</span> <span style="color:#4070a0;font-weight:bold">\
</span></span></span><span style="display:flex;"><span><span style="color:#4070a0;font-weight:bold"></span>    | jq -r <span style="color:#4070a0">&#39;.prefixes[] | [.ipv4Prefix, .service, .scope] | @csv&#39;</span> <span style="color:#4070a0;font-weight:bold">\
</span></span></span><span style="display:flex;"><span><span style="color:#4070a0;font-weight:bold"></span>      &gt; /tmp/vendor/google_raw_subnet_region.csv
</span></span><span style="display:flex;"><span>curl <span style="color:#4070a0">&#39;https://ip-ranges.amazonaws.com/ip-ranges.json&#39;</span> <span style="color:#4070a0;font-weight:bold">\
</span></span></span><span style="display:flex;"><span><span style="color:#4070a0;font-weight:bold"></span>    | jq -r <span style="color:#4070a0">&#39;.prefixes[] | [.ip_prefix, .service, .region] | @csv&#39;</span> <span style="color:#4070a0;font-weight:bold">\
</span></span></span><span style="display:flex;"><span><span style="color:#4070a0;font-weight:bold"></span>      &gt; /tmp/vendor/amazon_raw_subnet_region.csv
</span></span></code></pre></div><p>and the PeeringDB tables are downloaded via the API</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:2;-o-tab-size:2;tab-size:2;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>mkdir -p /tmp/peeringdb-tables
</span></span><span style="display:flex;"><span><span style="color:#bb60d5">PEERINGDB_TABLES</span><span style="color:#666">=(</span>
</span></span><span style="display:flex;"><span>    net
</span></span><span style="display:flex;"><span>    poc
</span></span><span style="display:flex;"><span><span style="color:#666">)</span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">for</span> PEERINGDB_TABLE in <span style="color:#70a0d0">${</span><span style="color:#bb60d5">PEERINGDB_TABLES</span>[*]<span style="color:#70a0d0">}</span>; <span style="color:#007020;font-weight:bold">do</span>
</span></span><span style="display:flex;"><span>    curl -sG <span style="color:#4070a0">&#34;https://www.peeringdb.com/api/</span><span style="color:#70a0d0">${</span><span style="color:#bb60d5">PEERINGDB_TABLE</span><span style="color:#70a0d0">}</span><span style="color:#4070a0">&#34;</span> | jq -c <span style="color:#4070a0">&#39;.data[]&#39;</span> | sed <span style="color:#4070a0">&#39;s,&#34;,\&#34;,g&#39;</span> &gt; <span style="color:#4070a0">&#34;/tmp/peeringdb-tables/</span><span style="color:#70a0d0">${</span><span style="color:#bb60d5">PEERINGDB_TABLE</span><span style="color:#70a0d0">}</span><span style="color:#4070a0">.json&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">done</span>
</span></span></code></pre></div><p>(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#load-into-bigquery-dataset-and-prepare-vendor-data">stage load into BigQuery dataset and prepare vendor data</a>)</p>
<p>The Potaroo ASN data is now joined with the PeeringDB data to add company name, website, and email.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:2;-o-tab-size:2;tab-size:2;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>copy (
</span></span><span style="display:flex;"><span>  select distinct asn.asn,
</span></span><span style="display:flex;"><span>  (net.data -&gt;&gt; &#39;name&#39;) as &#34;name&#34;,
</span></span><span style="display:flex;"><span>  (net.data -&gt;&gt; &#39;website&#39;) as &#34;website&#34;,
</span></span><span style="display:flex;"><span>  (poc.data -&gt;&gt; &#39;email&#39;) as email
</span></span><span style="display:flex;"><span>  from asnproc asn
</span></span><span style="display:flex;"><span>  left join peeriingdbnet net on (cast(net.data::jsonb -&gt;&gt; &#39;asn&#39; as bigint) = asn.asn)
</span></span><span style="display:flex;"><span>  left join peeriingdbpoc poc on ((poc.data -&gt;&gt; &#39;name&#39;) = (net.data -&gt;&gt; &#39;name&#39;))
</span></span><span style="display:flex;"><span>order by email asc) to &#39;/tmp/peeringdb_metadata_prepare.csv&#39; csv header;
</span></span></code></pre></div><p>It is then exported as CSV and uploaded to the BigQuery dataset.
(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#load-and-combine-peeringdb--potaroo-asn-data">stage load and combind PeeringDB + Potaroo ASN data</a>)</p>
<p>The Kubernetes Public Artifact Logs are then loaded into the BigQuery dataset in a table as the raw usage logs.
(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#load-kubernetes-public-artifact-traffic-logs-into-bigquery-from-gcs-bucket">stage load Kubernetes Public Artifact Traffic Logs into BigQuery from GCS bucket</a>)</p>
<p>Several queries are run against the BigQuery dataset to create some more handy tables, such as</p>
<ul>
<li>distinct IP count [from logs]; and</li>
<li>company name to ASN and IP block</li>
</ul>
<p>(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#create-tables-in-bigquery-for-use-in-datastudio-dashboard">stage create tables in BigQuery for use in DataStudio dashboard</a>)</p>
<p>With the heavy processing of the IPs done over in BigQuery, the data is pulled back through into Postgres with matches on IP to IP ranges. This is useful matching IP to IP range and then IP range to ASN.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:2;-o-tab-size:2;tab-size:2;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>copy
</span></span><span style="display:flex;"><span>  (
</span></span><span style="display:flex;"><span>    SELECT
</span></span><span style="display:flex;"><span>      vendor_expanded_int.cidr_ip,
</span></span><span style="display:flex;"><span>      vendor_expanded_int.start_ip,
</span></span><span style="display:flex;"><span>      vendor_expanded_int.end_ip,
</span></span><span style="display:flex;"><span>      vendor_expanded_int.asn,
</span></span><span style="display:flex;"><span>      vendor_expanded_int.name_with_yaml_name,
</span></span><span style="display:flex;"><span>      cust_ip.c_ip FROM vendor_expanded_int,
</span></span><span style="display:flex;"><span>      cust_ip
</span></span><span style="display:flex;"><span>    WHERE
</span></span><span style="display:flex;"><span>      cust_ip.c_ip &gt;= vendor_expanded_int.start_ip_int
</span></span><span style="display:flex;"><span>    AND
</span></span><span style="display:flex;"><span>      cust_ip.c_ip &lt;= vendor_expanded_int.end_ip_int
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>TO &#39;/tmp/match-ip-to-iprange.csv&#39; CSV HEADER;
</span></span></code></pre></div><p>The data is then pushed back to the BigQuery dataset, ready to be used.
(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#prepare-local-data-of-ip-to-ip-range-in-postgres">stage prepare local data of IP to IP range in Postgres</a>)</p>
<p>Addition tables in the BigQuery dataset are then made for joining</p>
<ul>
<li>usage data to IPs</li>
<li>company to IP</li>
</ul>
<p>with that all done, the dataset is complete.
(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#connect-all-the-data-in-the-dataset-of-bigquery-together">stage connect all the data in the dataset of BigQuery together</a>)</p>
<p>The last step of data processing is promoting the tables in the dataset to the <em>latest</em> / <em>stable</em> dataset which is picked up my DataStudio.
(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#prepare-local-data-of-ip-to-ip-range-in-postgres">stage override the existing data used in the DataStudio report</a>)</p>

<h2 id="datastudio-report">
  DataStudio report
  <a class = 'heading-anchor' href="#datastudio-report">#</a>
</h2>
<p>From the data produced, dashboards such as these can be made.</p>
<p>General stats</p>
<figure class="oversized">
<img  alt='k8s-infra-datastudio-repo-overview' src='/images/2021/k8s-infra-datastudio-repo-overview.png' />
</figure>
<p>IP count and GB of traffic</p>
<figure class="oversized">
<img alt='k8s-infra-datastudio-repo-ip-to-image' src='/images/2021/k8s-infra-datastudio-repo-ip-to-image.png'>
</figure>
<p>(Please note: IPs and vendors will not be visible in report)</p>
<p>Download cost per-image</p>
<figure class="oversized">
<img alt='k8s-infra-datastudio-repo-cost-per-image' src='/images/2021/k8s-infra-datastudio-repo-cost-per-image.png'>
</figure>

<h2 id="moving-forward">
  Moving forward
  <a class = 'heading-anchor' href="#moving-forward">#</a>
</h2>
<p>The Kubernetes Infra Working Group (soon SIG) begun the effort to migrate infrastructure off Google owned GCP projects some time ago.
Through the use of this data, ii + CNCF + Kubernetes Infra WG will be able to reduce costs by assisting organisations who are pulling heavily from Kubernetes Artifact services migrate to their own hosted services.</p>
<p>An action item for the community is to reduce the size of artifacts, see <a href="https://github.com/kubernetes/kubernetes/issues/102493">https://github.com/kubernetes/kubernetes/issues/102493</a>.</p>
<p>This container image is intended to be deployed into production as a Prow Job soon, so that the community can review its traffic cost per vendor, per IP, and per artifact.</p>

<h2 id="credits-and-special-thanks-to">
  Credits and special thanks to
  <a class = 'heading-anchor' href="#credits-and-special-thanks-to">#</a>
</h2>
<ul>
<li><a href="/author/berno-kleinhans">Berno Kleinhans</a> for data discovery, manipulation, and pipeline building</li>
<li><a href="/author/riaan-kleinhans">Riaan Kleinhans</a> for preparing the data and pulling it into a DataStudio report</li>
<li><a href="/author/zach-mandeville">Zach Mandeville</a> for being incredible at building out database queries</li>
<li>Arnaud Meukam and Aaron Crickenberger both for time and energy to support implementing the infrastructure and access to it, advice on our changes and approaches, and for merging in what ii was in need of for doing this Prow Job</li>
</ul>

    </div>
  </article>
  <nav class="back-nav">
  <a href="/post">‚Üê back to posts</a>
  </nav>
</section>

        </main>
        
<footer>
  <div class="footer__left">
  <b class='footer_logo'><a href="/">ii</a></b>
  </div>
  <div class='footer__right'>
  <ul class="contact-details">
 
    <li>   
      <a href="https://github.com/ii">
        <div id="github" class="icon">
            <i class="fab fa-github"></i>
        </div>
        Github
      </a>
    </li>
    <li>
      <a href="https://gitlab.com/ii">
        <div class="icon">
          <i class="fab fa-gitlab"></i>
        </div>
        Gitlab
      </a>
    </li>
    <li>
      <a href="mailto:hello@ii.nz">
        <div class="icon">
          <i class="fas fa-envelope"></i>
      </div>
      Email
      </a>
    </li>
    <li>
      <a href="https://www.linkedin.com/company/99165471/" target="_blank" rel="noopener noreferrer">
        <div class="icon">
          <i class="fab fa-linkedin"></i>
        </div>
        LinkedIn
      </a>
    </li>
    
  </ul>
</footer>
    </body>
</html>